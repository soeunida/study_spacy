{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 언어 모델 다운로드\r\n",
    "!python -m spacy download en_core_web_md"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "doc = nlp(\"I went there\")\r\n",
    "\r\n",
    "for token in doc:\r\n",
    "    print(token, type(token), token.text, type(token.text))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I <class 'spacy.tokens.token.Token'> I <class 'str'>\n",
      "went <class 'spacy.tokens.token.Token'> went <class 'str'>\n",
      "there <class 'spacy.tokens.token.Token'> there <class 'str'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "doc = nlp(\"I own a pretty cat.\")\r\n",
    "\r\n",
    "print ([ token.text for token in doc ], type([ token.text for token in doc ]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['I', 'own', 'a', 'pretty', 'cat', '.'] <class 'list'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "doc = nlp(\"It's been a crazy week!!!\")\r\n",
    "\r\n",
    "print ([ token.text for token in doc ], type([ token.text for token in doc ]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['It', \"'s\", 'been', 'a', 'crazy', 'week', '!', '!', '!'] <class 'list'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# custom tokenizer\r\n",
    "import spacy\r\n",
    "from spacy.symbols import ORTH # orthography를 의미 (맞춤법)\r\n",
    "\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "doc = nlp(\"lemme that\")\r\n",
    "print ([ token.text for token in doc ], type([ token.text for token in doc ]))\r\n",
    "\r\n",
    "special_case1 = [ {ORTH: \"lem\"}, {ORTH: \"me\"} ]\r\n",
    "special_case2 = [ {ORTH: \"Lem\"}, {ORTH: \"me\"} ]\r\n",
    "nlp.tokenizer.add_special_case(\"lemme\", special_case1)\r\n",
    "nlp.tokenizer.add_special_case(\"Lemme\", special_case2)\r\n",
    "doc = nlp(\"lemme that!!!\")\r\n",
    "print ([ token.text for token in doc ], type([ token.text for token in doc ]))\r\n",
    "\r\n",
    "doc = nlp(\"Let's try again! Lemme that!, lemme\")\r\n",
    "print ([ token.text for token in doc ], type([ token.text for token in doc ]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['lemme', 'that'] <class 'list'>\n",
      "['lem', 'me', 'that', '!', '!', '!'] <class 'list'>\n",
      "['Let', \"'s\", 'try', 'again', '!', 'Lem', 'me', 'that', '!', ',', 'lem', 'me'] <class 'list'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# custom tokenizer - 문장기호도 custom tokenizer에 포함될 수 있는 경우\r\n",
    "import spacy\r\n",
    "from spacy.symbols import ORTH # orthography를 의미 (맞춤법)\r\n",
    "\r\n",
    "special_case = [ {ORTH: \"...lemme...?\"} ]\r\n",
    "nlp.tokenizer.add_special_case(\"...lemme...?\", special_case)\r\n",
    "doc = nlp(\"I have a dream. ...lemme...?\")\r\n",
    "print ([ token.text for token in doc ], type([ token.text for token in doc ]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['I', 'have', 'a', 'dream', '.', '...lemme...?'] <class 'list'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "\r\n",
    "text = \"Let's go!\"\r\n",
    "doc = nlp(text)\r\n",
    "print ([ token.text for token in doc ])\r\n",
    "\r\n",
    "detail_tokens = nlp.tokenizer.explain(text)\r\n",
    "for detail_token in detail_tokens:\r\n",
    "    print(detail_token[1], \"\\t\", detail_token[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Let', \"'s\", 'go', '!']\n",
      "Let \t SPECIAL-1\n",
      "'s \t SPECIAL-2\n",
      "go \t TOKEN\n",
      "! \t SUFFIX\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit ('venv_20210725': venv)"
  },
  "interpreter": {
   "hash": "b2c46294d02003512898a675212ce284c66238a44ec8c080f36cf0db005bb14d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}